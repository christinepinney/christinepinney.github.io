import boto3
import os

output_path = os.getenv('CS535_S3_WORKSPACE')

client = boto3.client('emr')

client.run_job_flow(
    Name="my_cluster",
    # AWS will probably create a bucket like this for you. Change it to
    # your log bucket to view logs
    LogUri="s3://aws-logs-064794820934-us-west-2/elasticmapreduce/",
    ReleaseLabel="emr-7.2.0",
    Applications=[{"Name": "Spark"}, {"Name": "Hadoop"}],
    # Same as the interactive EMR
    Configurations=[
        {
            "Classification": "spark-env",
            "Configurations":[
                {
                    "Classification": "export",
                    "Properties": {
                        # Change this to an s3 bucket in your account to test your code
                        "CS535_S3_WORKSPACE": "s3://cpinney/p2/testing2/"
                        # "CS535_S3_WORKSPACE": output_path
                    },
                },
            ],
        },
        {
            "Classification": "spark",
            "Properties": {
            "maximizeResourceAllocation": "true"
            }
        },
        {
            "Classification": "spark-defaults",
            "Properties": {
            "spark.dynamicAllocation.executorIdleTimeout": "10800s",
            "spark.log4jHotPatch.enabled": "false",
            "spark.rdd.compress": "true",
            "spark.rpc.message.maxSize": "512",
            "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
            }
        },
        {
            "Classification": "core-site",
            "Properties": {
            "fs.s3n.multipart.uploads.split.size": "5368709120"
            }
        },
    ],
    Instances={
        'InstanceGroups': [
            {
                'Name': 'Primary',
                'Market': 'ON_DEMAND',
                'InstanceRole': 'MASTER',
                'InstanceType': 'c6g.xlarge',
                # This should always be 1, you only need a single primary
                'InstanceCount': 1,
            },
            {
                'Name': 'Workers',
                'Market': 'ON_DEMAND',
                'InstanceRole': 'CORE',
                'InstanceType': 'r6gd.2xlarge',
                'InstanceCount': 2,
            },
        ],
        'KeepJobFlowAliveWhenNoSteps': False,
        'TerminationProtected': False,
        'Ec2KeyName': 'cpinney',
	    'Ec2SubnetId': 'subnet-0cb7e22f3df454b26',
        # Replace these with security groups generated by EMR the first
        # time you started your cluster
        'AdditionalMasterSecurityGroups': ['sg-0b96bc35a0412c53e'],
        'AdditionalSlaveSecurityGroups': ['sg-0d051479bfb8f5e64'],

    },
    VisibleToAllUsers=True,
    # Replace these with your instance profile and your service role
    # that were automatically created by EMR
    JobFlowRole='arn:aws:iam::064794820934:instance-profile/AmazonEMR-InstanceProfile-20240920T100432',
    ServiceRole='arn:aws:iam::064794820934:role/service-role/AmazonEMR-ServiceRole-20240920T100449',
    Steps=[{
        "Name": "p2",
        "ActionOnFailure": "TERMINATE_CLUSTER",
        "HadoopJarStep": {
            "Jar": "command-runner.jar",
            "Args": [
                "spark-submit",
                "--deploy-mode", "client",
                "--master", "yarn",
                "--py-files", 
                "s3://cpinney/p2/p2.zip",
                "s3://cpinney/p2/entrypoint.py",
            ],
        },
    }]
    # BootstrapActions=[
    #     {
    #         'Name': 'string',
    #         'ScriptBootstrapAction': {
    #             'Path': 's3://cpinney/p2/install.sh',
    #             'Args': [
    #                 '',
    #             ]
    #         }
    #     },
    # ]
)
